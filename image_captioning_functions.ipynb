{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_img' from 'keras.preprocessing.image' (c:\\Users\\Thanushri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\preprocessing\\image.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Add\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCheckpoint\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_img, img_to_array\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg16\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VGG16, preprocess_input\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'load_img' from 'keras.preprocessing.image' (c:\\Users\\Thanushri\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\preprocessing\\image.py)"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from pickle import load\n",
    "\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys, time, os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1823138787.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip! install tqdm\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images in Dataset = 16182\n"
     ]
    }
   ],
   "source": [
    "image_path = \"D:\\Semester 6 Materials\\NLP\\Images (1).zip\"\n",
    "captions_english = \"D:\\Semester 6 Materials\\NLP\\captions_english.txt\"\n",
    "captions_hindi = \"D:\\Semester 6 Materials\\NLP\\captions_hindi.txt\"\n",
    "captions_malayalam = \"D:\\Semester 6 Materials\\NLP\\captions_malayalam.txt\"\n",
    "jpgs = os.listdir(image_path)\n",
    "print(\"Total Images in Dataset = {}\".format(len(jpgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>captions_english</th>\n",
       "      <th>captions_hindi</th>\n",
       "      <th>captions_malayalam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>a child in a pink dress is climbing up a set o...</td>\n",
       "      <td>गुलाबी पोशाक में एक बच्चा प्रवेश मार्ग में सीढ...</td>\n",
       "      <td>പിങ്ക് വസ്ത്രം ധരിച്ച ഒരു കുട്ടി പ്രവേശന വഴിയി...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>a girl going into a wooden building .</td>\n",
       "      <td>एक लड़की लकड़ी की इमारत में जा रही है।</td>\n",
       "      <td>ഒരു തടി കെട്ടിടത്തിലേക്ക് പോകുന്ന ഒരു പെൺകുട്ടി.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>a little girl climbing into a wooden playhouse .</td>\n",
       "      <td>एक छोटी लड़की लकड़ी के प्लेहाउस में चढ़ती है।</td>\n",
       "      <td>ഒരു കൊച്ചു പെൺകുട്ടി ഒരു തടി കളിസ്ഥലത്തേക്ക് ക...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>a little girl climbing the stairs to her playh...</td>\n",
       "      <td>एक छोटी सी लड़की अपने प्लेहाउस की सीढ़ियाँ चढ़...</td>\n",
       "      <td>ഒരു കൊച്ചു പെൺകുട്ടി തൻ്റെ കളിസ്ഥലത്തേക്കുള്ള ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>a little girl in a pink dress going into a woo...</td>\n",
       "      <td>गुलाबी पोशाक में एक छोटी लड़की लकड़ी के केबिन ...</td>\n",
       "      <td>പിങ്ക് വസ്ത്രം ധരിച്ച ഒരു കൊച്ചു പെൺകുട്ടി തടി...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    filename  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                    captions_english  \\\n",
       "0  a child in a pink dress is climbing up a set o...   \n",
       "1              a girl going into a wooden building .   \n",
       "2   a little girl climbing into a wooden playhouse .   \n",
       "3  a little girl climbing the stairs to her playh...   \n",
       "4  a little girl in a pink dress going into a woo...   \n",
       "\n",
       "                                      captions_hindi  \\\n",
       "0  गुलाबी पोशाक में एक बच्चा प्रवेश मार्ग में सीढ...   \n",
       "1             एक लड़की लकड़ी की इमारत में जा रही है।   \n",
       "2      एक छोटी लड़की लकड़ी के प्लेहाउस में चढ़ती है।   \n",
       "3  एक छोटी सी लड़की अपने प्लेहाउस की सीढ़ियाँ चढ़...   \n",
       "4  गुलाबी पोशाक में एक छोटी लड़की लकड़ी के केबिन ...   \n",
       "\n",
       "                                  captions_malayalam  \n",
       "0  പിങ്ക് വസ്ത്രം ധരിച്ച ഒരു കുട്ടി പ്രവേശന വഴിയി...  \n",
       "1   ഒരു തടി കെട്ടിടത്തിലേക്ക് പോകുന്ന ഒരു പെൺകുട്ടി.  \n",
       "2  ഒരു കൊച്ചു പെൺകുട്ടി ഒരു തടി കളിസ്ഥലത്തേക്ക് ക...  \n",
       "3  ഒരു കൊച്ചു പെൺകുട്ടി തൻ്റെ കളിസ്ഥലത്തേക്കുള്ള ...  \n",
       "4  പിങ്ക് വസ്ത്രം ധരിച്ച ഒരു കൊച്ചു പെൺകുട്ടി തടി...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "# Open the text files\n",
    "with open(captions_english, 'r', encoding='utf-8') as file:\n",
    "    text_english = file.read().splitlines()\n",
    "\n",
    "with open(captions_hindi, 'r', encoding='utf-8') as file:\n",
    "    text_hindi = file.read().splitlines()\n",
    "\n",
    "with open(captions_malayalam, 'r', encoding='utf-8') as file:\n",
    "    text_malayalam = file.read().splitlines()\n",
    "\n",
    "# Process the text\n",
    "datatxt = []\n",
    "for line_english, line_hindi, line_malayalam in zip_longest(text_english, text_hindi, text_malayalam, fillvalue=''):\n",
    "    col_english = line_english.split(',')\n",
    "    col_hindi = line_hindi.split(',')\n",
    "    col_malayalam = line_malayalam.split(',')\n",
    "    if len(col_english) != 2 or len(col_hindi) != 2 or len(col_malayalam) != 2:\n",
    "        continue\n",
    "    filename = col_english[0].strip()\n",
    "    captions_english = col_english[1].strip().lower()\n",
    "    captions_hindi = col_hindi[1].strip().lower()\n",
    "    captions_malayalam = col_malayalam[1].strip().lower()\n",
    "\n",
    "    # Add the data to the list\n",
    "    datatxt.append([filename, captions_english, captions_hindi, captions_malayalam])\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame(datatxt, columns=[\"filename\", \"captions_english\", \"captions_hindi\", \"captions_malayalam\"])\n",
    "uni_filenames = data[\"filename\"].unique()\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size of english: 8514\n",
      "Vocabulary Size of hindi: 7881\n",
      "Vocabulary Size of malayalam: 18010\n"
     ]
    }
   ],
   "source": [
    "## Vocabulary Size \n",
    "vocabulary_english = []\n",
    "for txt in data.captions_english.values:\n",
    "   vocabulary_english.extend(txt.split())\n",
    "print('Vocabulary Size of english: %d' % len(set(vocabulary_english)))\n",
    "\n",
    "vocabulary_hindi = []\n",
    "for txt in data.captions_hindi.values:\n",
    "   vocabulary_hindi.extend(txt.split())\n",
    "print('Vocabulary Size of hindi: %d' % len(set(vocabulary_hindi)))\n",
    "\n",
    "vocabulary_malayalam=[]\n",
    "for txt in data.captions_malayalam.values:\n",
    "   vocabulary_malayalam.extend(txt.split())\n",
    "print('Vocabulary Size of malayalam: %d' % len(set(vocabulary_malayalam)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punctuation(text_original):\n",
    "   text_no_punctuation = text_original.translate(string.punctuation)\n",
    "   return(text_no_punctuation)\n",
    "\n",
    "def remove_single_character(text):\n",
    "   text_len_more_than1 = \"\"\n",
    "   for word in text.split():\n",
    "       if len(word) > 1:\n",
    "           text_len_more_than1 += \" \" + word\n",
    "   return(text_len_more_than1)\n",
    "\n",
    "def remove_numeric(text):\n",
    "   text_no_numeric = \"\"\n",
    "   for word in text.split():\n",
    "       isalpha = word.isalpha()\n",
    "       if isalpha:\n",
    "           text_no_numeric += \" \" + word\n",
    "   return(text_no_numeric)\n",
    "\n",
    "def text_clean(text_original):\n",
    "   text = remove_punctuation(text_original)\n",
    "   text = remove_single_character(text)\n",
    "   text = remove_numeric(text)\n",
    "   return(text)\n",
    "\n",
    "for i, caption in enumerate(data.captions_english.values):\n",
    "   newcaption = text_clean(caption)\n",
    "   data[\"captions_english\"].iloc[i] = newcaption\n",
    "   #data[\"captions_hindi\"].oloc[i] = newcaption\n",
    "   #data[\"captions_malayalam\"].oloc[i] = newcaption\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Adding tags <start> <end>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> പിങ്ക് വസ്ത്രം ധരിച്ച ഒരു കുട്ടി പ്രവേശന വഴിയിൽ ഒരു കൂട്ടം പടികൾ കയറുന്നു. <end>',\n",
       " '<start> ഒരു തടി കെട്ടിടത്തിലേക്ക് പോകുന്ന ഒരു പെൺകുട്ടി. <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_captions_english = []\n",
    "for caption  in data[\"captions_english\"].astype(str):\n",
    "   caption = '<start> ' + caption+ ' <end>'\n",
    "   all_captions_english.append(caption)\n",
    "\n",
    "all_captions_english[:10]\n",
    "\n",
    "\n",
    "all_captions_hindi = []\n",
    "for caption  in data[\"captions_hindi\"].astype(str):\n",
    "   caption = '<start> ' + caption+ ' <end>'\n",
    "   all_captions_hindi.append(caption)\n",
    "\n",
    "all_captions_hindi[:2]\n",
    "\n",
    "\n",
    "all_captions_malayalam = []\n",
    "for caption  in data[\"captions_malayalam\"].astype(str):\n",
    "   caption = '<start> ' + caption+ ' <end>'\n",
    "   all_captions_malayalam.append(caption)\n",
    "\n",
    "all_captions_malayalam[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/Madhu/Downloads/Images/1000268201_693b08cb0e.jpg',\n",
       " 'C:/Users/Madhu/Downloads/Images/1000268201_693b08cb0e.jpg',\n",
       " 'C:/Users/Madhu/Downloads/Images/1000268201_693b08cb0e.jpg',\n",
       " 'C:/Users/Madhu/Downloads/Images/1000268201_693b08cb0e.jpg',\n",
       " 'C:/Users/Madhu/Downloads/Images/1000268201_693b08cb0e.jpg',\n",
       " 'C:/Users/Madhu/Downloads/Images/1001773457_577c3a7d70.jpg',\n",
       " 'C:/Users/Madhu/Downloads/Images/1001773457_577c3a7d70.jpg',\n",
       " 'C:/Users/Madhu/Downloads/Images/1001773457_577c3a7d70.jpg',\n",
       " 'C:/Users/Madhu/Downloads/Images/1001773457_577c3a7d70.jpg',\n",
       " 'C:/Users/Madhu/Downloads/Images/1001773457_577c3a7d70.jpg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img_name_vector = []\n",
    "for annot in data[\"filename\"]:\n",
    "   full_image_path = image_path +\"/\"+ annot\n",
    "   all_img_name_vector.append(full_image_path)\n",
    "\n",
    "all_img_name_vector[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_img_name_vector) : 37212\n",
      "len(all_captions_english) : 37212\n",
      "len(all_captions_hindi) : 37212\n",
      "len(all_captions_malayalam) : 37212\n",
      "{111636}\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(all_img_name_vector) : {len(all_img_name_vector)}\")\n",
    "print(f\"len(all_captions_english) : {len(all_captions_english)}\")\n",
    "print(f\"len(all_captions_hindi) : {len(all_captions_hindi)}\")\n",
    "print(f\"len(all_captions_malayalam) : {len(all_captions_malayalam)}\")\n",
    "all_captions_size = {len(all_captions_english)+len(all_captions_hindi)+len(all_captions_malayalam)}\n",
    "print(all_captions_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_captions_english: 37200\n",
      "Length of train_captions_hindi: 37200\n",
      "Length of train_captions_malayalam: 37200\n",
      "Length of img_name_vector: 37200\n",
      "Example img_name_vector: ['C:/Users/Madhu/Downloads/Images/2120469056_7a738413be.jpg', 'C:/Users/Madhu/Downloads/Images/2842609837_b3a0b383f7.jpg', 'C:/Users/Madhu/Downloads/Images/3470129475_9e58b6742c.jpg', 'C:/Users/Madhu/Downloads/Images/167295035_336f5f5f27.jpg', 'C:/Users/Madhu/Downloads/Images/159712188_d530dd478c.jpg']\n"
     ]
    }
   ],
   "source": [
    "def data_limiter(num, total_captions_english, total_captions_hindi, total_captions_malayalam, all_img_name_vector):\n",
    "    # Shuffle the captions and image names\n",
    "    train_captions_english, train_captions_hindi, train_captions_malayalam, img_name_vector = shuffle(\n",
    "        total_captions_english, total_captions_hindi, total_captions_malayalam, all_img_name_vector, random_state=1\n",
    "    )\n",
    "    \n",
    "    # Limit the data to the specified number\n",
    "    train_captions_english = train_captions_english[:num]\n",
    "    train_captions_hindi = train_captions_hindi[:num]\n",
    "    train_captions_malayalam = train_captions_malayalam[:num]\n",
    "    img_name_vector = img_name_vector[:num]\n",
    "    \n",
    "    print(\"Length of train_captions_english:\", len(train_captions_english))\n",
    "    print(\"Length of train_captions_hindi:\", len(train_captions_hindi))\n",
    "    print(\"Length of train_captions_malayalam:\", len(train_captions_malayalam))\n",
    "    print(\"Length of img_name_vector:\", len(img_name_vector))\n",
    "    print(\"Example img_name_vector:\", img_name_vector[:5])  # Print first 5 elements for inspection\n",
    "    \n",
    "    return train_captions_english, train_captions_hindi, train_captions_malayalam, img_name_vector\n",
    "\n",
    "train_captions_english, train_captions_hindi, train_captions_malayalam, img_name_vector = data_limiter(37200, all_captions_english, all_captions_hindi, all_captions_malayalam, all_img_name_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model - VGG16 encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Madhu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Madhu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, None, 3)]   0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 14714688 (56.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def load_image(image_path):\n",
    "   img = tf.io.read_file(image_path)\n",
    "   img = tf.image.decode_jpeg(img, channels=3)\n",
    "   img = tf.image.resize(img, (224, 224))\n",
    "   img = preprocess_input(img)\n",
    "   return img, image_path\n",
    "\n",
    "image_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "\n",
    "image_features_extract_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n",
      "<_BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>\n"
     ]
    }
   ],
   "source": [
    "encode_train = sorted(set(img_name_vector))\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "print(image_dataset)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)\n",
    "print(image_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#%%time\\nfor img, path in tqdm(image_dataset):\\n batch_features = image_features_extract_model(img)\\n batch_features = tf.reshape(batch_features,\\n                             (batch_features.shape[0], -1, batch_features.shape[3]))\\n\\n for bf, p in zip(batch_features, path):\\n   path_of_feature = p.numpy().decode(\"utf-8\")\\n   np.save(path_of_feature, bf.numpy())\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '''\n",
    "# #%%time\n",
    "# for img, path in tqdm(image_dataset):\n",
    "#  batch_features = image_features_extract_model(img)\n",
    "#  batch_features = tf.reshape(batch_features,\n",
    "#                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "#  for bf, p in zip(batch_features, path):\n",
    "#    path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "#    np.save(path_of_feature, bf.numpy())\n",
    "# '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k=5000\n",
    "# Tokenize English captions\n",
    "tokenizer_english = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                           oov_token=\"<unk>\",\n",
    "                                                           filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer_english.fit_on_texts(train_captions_english)\n",
    "train_seqs_english = tokenizer_english.texts_to_sequences(train_captions_english)\n",
    "tokenizer_english.word_index['<pad>'] = 0\n",
    "tokenizer_english.index_word[0] = '<pad>'\n",
    "cap_vector_english = tf.keras.preprocessing.sequence.pad_sequences(train_seqs_english, padding='post')\n",
    "\n",
    "# Tokenize Hindi captions\n",
    "tokenizer_hindi = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                         oov_token=\"<unk>\",\n",
    "                                                         filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer_hindi.fit_on_texts(train_captions_hindi)\n",
    "train_seqs_hindi = tokenizer_hindi.texts_to_sequences(train_captions_hindi)\n",
    "tokenizer_hindi.word_index['<pad>'] = 0\n",
    "tokenizer_hindi.index_word[0] = '<pad>'\n",
    "cap_vector_hindi = tf.keras.preprocessing.sequence.pad_sequences(train_seqs_hindi, padding='post')\n",
    "\n",
    "# Tokenize Malayalam captions\n",
    "tokenizer_malayalam = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                             oov_token=\"<unk>\",\n",
    "                                                             filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer_malayalam.fit_on_texts(train_captions_malayalam)\n",
    "train_seqs_malayalam = tokenizer_malayalam.texts_to_sequences(train_captions_malayalam)\n",
    "tokenizer_malayalam.word_index['<pad>'] = 0\n",
    "tokenizer_malayalam.index_word[0] = '<pad>'\n",
    "cap_vector_malayalam = tf.keras.preprocessing.sequence.pad_sequences(train_seqs_malayalam, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Captions:\n",
      "<start> two children swimming in green water . <end>\n",
      "<start> a group of four friends are drinking alcohol at a party . <end>\n",
      "<start> a man jumps his bicycle high above a cement incline . <end>\n",
      "\n",
      "Hindi Captions:\n",
      "<start> दो बच्चे हरे पानी में तैर रहे हैं। <end>\n",
      "<start> चार दोस्तों का एक समूह एक पार्टी में शराब पी रहा है। <end>\n",
      "<start> एक आदमी अपनी साइकिल को सीमेंट की ढलान से ऊपर कूदता है। <end>\n",
      "\n",
      "Malayalam Captions:\n",
      "<start> രണ്ട് കുട്ടികൾ പച്ചവെള്ളത്തിൽ നീന്തുന്നു. <end>\n",
      "<start> നാല് സുഹൃത്തുക്കളുടെ ഒരു സംഘം ഒരു പാർട്ടിയിൽ മദ്യം കഴിക്കുന്നു. <end>\n",
      "<start> ഒരു മനുഷ്യൻ തൻ്റെ സൈക്കിൾ സിമൻ്റ് ചരിവിന് മുകളിൽ ചാടുന്നു. <end>\n"
     ]
    }
   ],
   "source": [
    "# English captions\n",
    "print(\"English Captions:\")\n",
    "for caption in train_captions_english[:3]:\n",
    "    print(caption)\n",
    "\n",
    "# Hindi captions\n",
    "print(\"\\nHindi Captions:\")\n",
    "for caption in train_captions_hindi[:3]:\n",
    "    print(caption)\n",
    "\n",
    "# Malayalam captions\n",
    "print(\"\\nMalayalam Captions:\")\n",
    "for caption in train_captions_malayalam[:3]:\n",
    "    print(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First three tokenized sequences (English):\n",
      "[3, 14, 60, 130, 5, 58, 25, 4]\n",
      "[3, 2, 56, 13, 119, 1085, 18, 434, 3854, 23, 2, 674, 4]\n",
      "[3, 2, 12, 68, 30, 139, 188, 232, 2, 515, 1675, 4]\n",
      "\n",
      "First three tokenized sequences (Hindi):\n",
      "[3, 16, 46, 98, 31, 7, 251, 21, 10, 4]\n",
      "[3, 140, 1665, 23, 2, 74, 2, 714, 7, 531, 336, 9, 5, 4]\n",
      "[3, 2, 15, 69, 84, 20, 557, 12, 418, 14, 83, 59, 5, 4]\n",
      "\n",
      "First three tokenized sequences (Malayalam):\n",
      "[3, 7, 36, 4612, 226, 4]\n",
      "[3, 95, 1, 2, 391, 2, 1297, 3333, 265, 4]\n",
      "[3, 2, 8, 73, 56, 854, 2668, 63, 17, 4]\n"
     ]
    }
   ],
   "source": [
    "# English captions\n",
    "print(\"First three tokenized sequences (English):\")\n",
    "for seq in train_seqs_english[:3]:\n",
    "    print(seq)\n",
    "\n",
    "# Hindi captions\n",
    "print(\"\\nFirst three tokenized sequences (Hindi):\")\n",
    "for seq in train_seqs_hindi[:3]:\n",
    "    print(seq)\n",
    "\n",
    "# Malayalam captions\n",
    "print(\"\\nFirst three tokenized sequences (Malayalam):\")\n",
    "    \n",
    "for seq in train_seqs_malayalam[:3]:\n",
    "    print(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "3\n",
      "Max Length of english caption : Min Length of english caption = 58 : 3\n",
      "124\n",
      "3\n",
      "Max Length of hindi caption : Min Length of hindi caption = 124 : 3\n",
      "58\n",
      "3\n",
      "Max Length of malayalam caption : Min Length of malayalam caption = 58 : 3\n"
     ]
    }
   ],
   "source": [
    "def calc_max_length(tensor):\n",
    "   return max(len(t) for t in tensor)\n",
    "\n",
    "\n",
    "def calc_min_length(tensor):\n",
    "   return min(len(t) for t in tensor)\n",
    "\n",
    "max_length_english = calc_max_length(train_seqs_malayalam)\n",
    "min_length_malayalam = calc_min_length(train_seqs_malayalam)\n",
    "\n",
    "print(max_length_english)\n",
    "print(min_length_malayalam)\n",
    "print('Max Length of english caption : Min Length of english caption = '+ str(max_length_english) +\" : \"+str(min_length_malayalam))\n",
    "\n",
    "max_length_hindi = calc_max_length(train_seqs_hindi)\n",
    "min_length_hindi = calc_min_length(train_seqs_hindi)\n",
    "\n",
    "print(max_length_hindi)\n",
    "print(min_length_hindi)\n",
    "print('Max Length of hindi caption : Min Length of hindi caption = '+ str(max_length_hindi) +\" : \"+str(min_length_hindi))\n",
    "\n",
    "max_length_malayalam = calc_max_length(train_seqs_malayalam)\n",
    "min_length_malayalam = calc_min_length(train_seqs_malayalam)\n",
    "\n",
    "print(max_length_malayalam)\n",
    "print(min_length_malayalam)\n",
    "print('Max Length of malayalam caption : Min Length of malayalam caption = '+ str(max_length_malayalam) +\" : \"+str(min_length_malayalam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_train, img_name_val, cap_train_english, cap_val_english = train_test_split(img_name_vector,cap_vector_english, test_size=0.2, random_state=0)\n",
    "img_name_train, img_name_val, cap_train_hindi, cap_val_hindi = train_test_split(img_name_vector,cap_vector_hindi, test_size=0.2, random_state=0)\n",
    "img_name_train, img_name_val, cap_train_malayalam, cap_val_malayalam = train_test_split(img_name_vector,cap_vector_malayalam, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size_english = len(tokenizer_english.word_index) + 1\n",
    "vocab_size_hindi = len(tokenizer_hindi.word_index) + 1\n",
    "vocab_size_malayalam = len(tokenizer_malayalam.word_index) + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "features_shape = 512\n",
    "attention_features_shape = 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_func(img_name, cap):\n",
    " img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    " return img_tensor, cap\n",
    "\n",
    "dataset_english = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train_english))\n",
    "dataset_english = dataset_english.map(lambda item1, item2: tf.numpy_function(\n",
    "        map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_english = dataset_english.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset_english = dataset_english.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "dataset_hindi = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train_hindi))\n",
    "dataset_hindi = dataset_hindi.map(lambda item1, item2: tf.numpy_function(\n",
    "        map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_hindi = dataset_hindi.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset_hindi = dataset_hindi.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "dataset_malayalam = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train_malayalam))\n",
    "dataset_malayalam = dataset_malayalam.map(lambda item1, item2: tf.numpy_function(\n",
    "        map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_malayalam = dataset_malayalam.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset_malayalam = dataset_malayalam.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab_size = len(vocabulary_english)\n",
    "hindi_vocab_size = len(vocabulary_hindi)\n",
    "malayalam_vocab_size = len(vocabulary_malayalam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "   from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    " mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    " loss_ = loss_object(real, pred)\n",
    " mask = tf.cast(mask, dtype=loss_.dtype)\n",
    " loss_ *= mask\n",
    "\n",
    " return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_Encoder(tf.keras.Model):\n",
    "   # This encoder passes the features through a Fully connected layer\n",
    "   def __init__(self, embedding_dim):\n",
    "       super(VGG16_Encoder, self).__init__()\n",
    "       # shape after fc == (batch_size, 49, embedding_dim)\n",
    "       self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "       self.dropout = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)\n",
    "\n",
    "   def call(self, x):\n",
    "       #x= self.dropout(x)\n",
    "       x = self.fc(x)\n",
    "       x = tf.nn.relu(x)\n",
    "       return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rnn_Local_Decoder(tf.keras.Model):\n",
    " def __init__(self, embedding_dim, units, vocab_size):\n",
    "   super(Rnn_Local_Decoder, self).__init__()\n",
    "   self.units = units\n",
    "   self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "   self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                  return_sequences=True,\n",
    "                                  return_state=True,\n",
    "                                  recurrent_initializer='glorot_uniform')\n",
    "  \n",
    "   self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "\n",
    "   self.dropout = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)\n",
    "   self.batchnormalization = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
    "\n",
    "   self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "   self.Uattn = tf.keras.layers.Dense(units)\n",
    "   self.Wattn = tf.keras.layers.Dense(units)\n",
    "   self.Vattn = tf.keras.layers.Dense(1)\n",
    "\n",
    " def call(self, x, features, hidden):\n",
    "   hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "   score = self.Vattn(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)))\n",
    "   attention_weights = tf.nn.softmax(score, axis=1)\n",
    "   context_vector = attention_weights * features\n",
    "   context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "   x = self.embedding(x)\n",
    "   x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "   output, state = self.gru(x)\n",
    "   x = self.fc1(output)\n",
    "   x = tf.reshape(x, (-1, x.shape[2]))\n",
    "   x= self.dropout(x)\n",
    "   x= self.batchnormalization(x)\n",
    "   x = self.fc2(x)\n",
    "   return x, state, attention_weights\n",
    "\n",
    " def reset_state(self, batch_size):\n",
    "   return tf.zeros((batch_size, self.units))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = VGG16_Encoder(embedding_dim)\n",
    "decoder_english = Rnn_Local_Decoder(embedding_dim, units, vocab_size_english)\n",
    "decoder_hindi = Rnn_Local_Decoder(embedding_dim, units, vocab_size_hindi)\n",
    "decoder_malayalam = Rnn_Local_Decoder(embedding_dim, units, vocab_size_malayalam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target,decoder,tokenizer):\n",
    " loss = 0\n",
    " # initializing the hidden state for each batch\n",
    " # because the captions are not related from image to image\n",
    "\n",
    " hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    " dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    " with tf.GradientTape() as tape:\n",
    "     features = encoder(img_tensor)\n",
    "     for i in range(1, target.shape[1]):\n",
    "         # passing the features through the decoder\n",
    "         predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "         loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "         # using teacher forcing\n",
    "         dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    " total_loss = (loss / int(target.shape[1]))\n",
    " trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    " gradients = tape.gradient(loss, trainable_variables)\n",
    " optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    " return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image,max_length,decoder,tokenizer):\n",
    "   attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "   hidden = decoder.reset_state(batch_size=1)\n",
    "   temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "   img_tensor_val = image_features_extract_model(temp_input)\n",
    "   img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "   features = encoder(img_tensor_val)\n",
    "   dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "   result = []\n",
    "\n",
    "   for i in range(max_length):\n",
    "       predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "       attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "       predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "       result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "       if tokenizer.index_word[predicted_id] == '<end>':\n",
    "           return result, attention_plot\n",
    "\n",
    "       dec_input = tf.expand_dims([predicted_id], 0)\n",
    "   attention_plot = attention_plot[:len(result), :]\n",
    "\n",
    "   return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "   temp_image = np.array(Image.open(image))\n",
    "   fig = plt.figure(figsize=(10, 10))\n",
    "   len_result = len(result)\n",
    "   for l in range(len_result):\n",
    "       temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "       ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "       ax.set_title(result[l])\n",
    "       img = ax.imshow(temp_image)\n",
    "       ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.1814\n",
      "Epoch 1 Batch 100 Loss 1.7318\n",
      "Epoch 1 Batch 200 Loss 1.4597\n",
      "Epoch 1 Batch 300 Loss 1.3066\n",
      "Epoch 1 Batch 400 Loss 1.1682\n",
      "Epoch 1 Loss 1.529875\n",
      "Time taken for 1 epoch 1451.4484753608704 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "loss_plot_english = []\n",
    "start_epoch = 0\n",
    "EPOCHS = 20\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target_english)) in enumerate(dataset_english):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target_english,decoder_english,tokenizer_english)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, batch_loss.numpy() / int(target_english.shape[1])))\n",
    "\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot_english.append(total_loss / num_steps)\n",
    "\n",
    "    print('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss / num_steps))\n",
    "\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #saving the weights\n",
    "# decoder_english.save_weights('decoder_english_weights.h5')\n",
    "\n",
    "# # Save the loss plot\n",
    "# with open('loss_plot_english.pkl', 'wb') as f:\n",
    "# pickle.dump(loss_plot_english, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the model weights\n",
    "decoder_english.load_weights('decoder_english_weights.h5')\n",
    "\n",
    "# Load the loss plot\n",
    "with open('loss_plot_english.pkl', 'rb') as f:\n",
    "    loss_plot_english = pickle.load(f)\n",
    "\n",
    "# Print the loss plot or use it as needed\n",
    "print(\"Loaded Loss Plot:\", loss_plot_english)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hindi Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_plot_hindi = []\n",
    "# start_epoch = 0\n",
    "# EPOCHS = 20\n",
    "# for epoch in range(start_epoch, EPOCHS):\n",
    "#     start = time.time()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for (batch, (img_tensor, target_hindi)) in enumerate(dataset_hindi):\n",
    "#         batch_loss, t_loss = train_step(img_tensor, target_hindi,decoder_hindi,tokenizer_hindi)\n",
    "#         total_loss += t_loss\n",
    "\n",
    "#         if batch % 100 == 0:\n",
    "#             print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "#                 epoch + 1, batch, batch_loss.numpy() / int(target_hindi.shape[1])))\n",
    "\n",
    "#     # storing the epoch end loss value to plot later\n",
    "#     loss_plot_english.append(total_loss / num_steps)\n",
    "\n",
    "#     print('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "#                                          total_loss / num_steps))\n",
    "\n",
    "#     print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the weights\n",
    "decoder_hindi.save_weights('decoder_hindi_weights.h5')\n",
    "\n",
    "# Save the loss plot\n",
    "with open('loss_plot_hindi.pkl', 'wb') as f:\n",
    "    pickle.dump(loss_plot_hindi, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the model weights\n",
    "decoder_hindi.load_weights('decoder_hindi_weights.h5')\n",
    "\n",
    "# Load the loss plot\n",
    "with open('loss_plot_hindi.pkl', 'rb') as f:\n",
    "    loss_plot_hindi = pickle.load(f)\n",
    "\n",
    "# Print the loss plot or use it as needed\n",
    "print(\"Loaded Loss Plot:\", loss_plot_hindi)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malayalam Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot_malayalam = []\n",
    "start_epoch = 0\n",
    "EPOCHS = 20\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target_malayalam)) in enumerate(dataset_malayalam):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target_malayalam,decoder_malayalam,tokenizer_malayalam)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, batch_loss.numpy() / int(target_malayalam.shape[1])))\n",
    "\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot_malayalam.append(total_loss / num_steps)\n",
    "\n",
    "    print('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss / num_steps))\n",
    "\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the weights\n",
    "decoder_malayalam.save_weights('decoder_malayalam_weights.h5')\n",
    "\n",
    "# Save the loss plot\n",
    "with open('loss_plot_malayalam.pkl', 'wb') as f:\n",
    "    pickle.dump(loss_plot_malayalam, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
